# 关于
# 数据集

优先使用T2Ranking和MS-MARCO数据集做测试。

1. T2Ranking 中文数据集, https://github.com/THUIR/T2Ranking/
  **核心信息：** 由清华大学团队构建的大规模中文段落排序基准数据集，支持召回（第一阶段检索）和重排序任务，包含 30 万 + 真实查询、200 万 + 唯一段落，所有查询 - 段落对均由专家标注 4 级相关性分数（细粒度标注，避免二分类的粗糙性）。
  **适配性：** 数据源自真实搜索引擎，每个段落均对应原始文档的 XML 文件（含文档标题、来源信息等），可提取 “文档标题 - 段落内容”“文件名（可基于文档 ID / 主题生成规范文件名）- 段落内容” 的关联对，测试标题 / 文件名对相关段落的召回效果。
  **优势：** 解决中文数据集规模小、标注粗糙的问题，测试集包含高多样性段落，减少假阴性标注，评估结果更准确；支持 Apache License 2.0 开源，可直接下载使用（地址：https://github.com/THUIR/T2Ranking/）。
  **适用场景：** 通用领域中文标题 / 文件名召回测试，尤其适合验证语义匹配类召回方法（如向量检索）。
1. MS-MARCO 数据集
  **核心信息：** 微软推出的大规模 NLP 数据集，涵盖机器阅读理解、信息检索等任务，分为基础版和扩展版：
  **基础版（2016）：** 含 10 万个真实用户查询（来自 Bing 和 Cortana），答案基于真实网页内容人工编写，可提取 “网页标题 - 文档内容” 关联对；
  **扩展版（MS MARCO Web Search，2024）：** 含 10 亿个高质量网页（源自 ClueWeb22）、1000 万独特查询，支持 207 种语言，提供 HTML 结构、语义标注等信息，可自定义生成 “文件名（基于网页主题 / ID）- 文档内容” 映射。
  **适配性：** 数据规模庞大，查询和文档均来自真实场景，可模拟 “通过标题 / 文件名召回相关文档” 的真实检索需求；支持稀疏检索（如 BM25）和稠密检索（向量检索）的召回效果测试。
  **优势：** 行业认可度高，被视为信息检索领域的 “ImageNet”，数据多样性强，可测试召回方法的泛化能力；提供不同规模版本（100M/10B），适配不同实验资源。
  **适用场景：** 通用领域英文标题 / 文件名召回测试，尤其适合大规模数据集场景。
1. TREC DL19/20 数据集
  **核心信息：** TREC（文本检索会议）推出的深度学习赛道数据集，专注于文档 / 段落检索任务，数据规模巨大（数百万文档），包含真实搜索查询和相关性判断标注，是检验检索模型性能的权威基准。
  **适配性：** 支持 “标题 - 文档”“文件名 - 文档” 的召回测试，常被用于验证召回算法的有效性（如 BM25、向量检索模型 Contriever 等均在该数据集上有基准测试结果）。
  **优势：** 评测标准严格，相关性标注权威，可与现有 SOTA 方法对比实验结果；数据覆盖多领域，避免单一场景的局限性。
  **适用场景：** 学术实验、召回算法性能对比，尤其适合验证方法的先进性。
